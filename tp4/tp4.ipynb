{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Première analyse : Mots-clés et nuage de mots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yake\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le répertoire qui contient vos fichiers txt exportés de Camille\n",
    "indir = \"../data/camille_vaccination_/\"\n",
    "# Le répertoire qui contiendra les fichiers txt nettoyés\n",
    "outdir = \"../data/txt_clean2\"\n",
    "\n",
    "if not os.path.exists(outdir):\n",
    "    os.mkdir(outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = [\"vaccination\",\"vaccin\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une regex afin de trouver les mots de la liste query dans le texte\n",
    "regex = re.compile(f\"\\\\b({'|'.join(query)})\\\\b\", re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(indir)[:150]:\n",
    "    if file.endswith(\".txt\"):\n",
    "        relevant_sentences = []\n",
    "        f_in = open(os.path.join(indir, file), encoding=\"utf-8\")\n",
    "        text = f_in.read()\n",
    "        for sentence in sent_tokenize(text):\n",
    "            if regex.search(sentence):\n",
    "                relevant_sentences.append(sentence)\n",
    "        f_in.close()\n",
    "        f_out = open(os.path.join(outdir, file), \"w\", encoding=\"utf-8\")\n",
    "        f_out.write(\"\\n\\n\".join(relevant_sentences))\n",
    "        f_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantier l'extracteur de mots clés\n",
    "kw_extractor = yake.KeywordExtractor(lan=\"fr\", top=50)\n",
    "kw_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lister les Fichiers\n",
    "data_path = \"../data/txt_clean2\"\n",
    "files = os.listdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in sorted(files)[:1000]:\n",
    "    text = open(os.path.join(data_path, f), 'r', encoding=\"utf-8\").read()\n",
    "    keywords = kw_extractor.extract_keywords(text)\n",
    "    kept = []\n",
    "    for kw, score in keywords:\n",
    "        words = kw.split()\n",
    "        if len(words) == 2:\n",
    "            kept.append(kw)\n",
    "    print(f\"{f} mentions these keywords: {', '.join(kept)}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nuage de mots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import os\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords (Idem que dans s1)\n",
    "sw = stopwords.words(\"french\")\n",
    "sw += [\"les\", \"plus\", \"cette\", \"fait\", \"faire\", \"être\", \"deux\", \"comme\", \"dont\", \"tout\",\n",
    "       \"ils\", \"bien\", \"sans\", \"peut\", \"tous\", \"après\", \"ainsi\", \"donc\", \"cet\", \"sous\",\n",
    "       \"celle\", \"entre\", \"encore\", \"toutes\", \"pendant\", \"moins\", \"dire\", \"cela\", \"non\",\n",
    "       \"faut\", \"trois\", \"aussi\", \"dit\", \"avoir\", \"doit\", \"contre\", \"depuis\", \"autres\",\n",
    "       \"van\", \"het\", \"autre\", \"jusqu\", \"ville\", \"rossel\", \"dem\",\"rue\", \"villa\",\"louer\",\n",
    "       \"maison\",\"fr\",\"app\",\"ch\",\"vendre\",\"prix\",\"bel\",\"av\",\"jard\",\"Libre\",\"jardin\",\"vue\",\n",
    "       \"téléphone\",\"centre\",\"terrain\"]\n",
    "sw = set(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "\n",
    "# Télécharger les stopwords si ce n'est pas déjà fait\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Liste des stopwords du français\n",
    "sw = set(stopwords.words(\"french\"))\n",
    "\n",
    "# Ajout de mots personnalisés à la liste des stopwords\n",
    "custom_sw = [\n",
    "    \"les\", \"plus\", \"cette\", \"fait\", \"faire\", \"être\", \"deux\", \"comme\", \"dont\", \"tout\",\n",
    "    \"ils\", \"bien\", \"sans\", \"peut\", \"tous\", \"après\", \"ainsi\", \"donc\", \"cet\", \"sous\",\n",
    "    \"celle\", \"entre\", \"encore\", \"toutes\", \"pendant\", \"moins\", \"dire\", \"cela\", \"non\",\n",
    "    \"faut\", \"trois\", \"aussi\", \"dit\", \"avoir\", \"doit\", \"contre\", \"depuis\", \"autres\",\n",
    "    \"van\", \"het\", \"autre\", \"jusqu\", \"ville\", \"rossel\", \"dem\", \"rue\", \"villa\", \"louer\",\n",
    "    \"maison\", \"fr\", \"app\", \"ch\", \"vendre\", \"prix\", \"bel\", \"av\", \"jard\", \"Libre\", \"jardin\", \"do\"\n",
    "    \"a\", \"vue\", \"téléphone\", \"centre\", \"terrain\", \"déjà\",\"ot\",\"u\",\"qu'il\",\"or\", \"qu'une\",\"l'a\"\n",
    "]\n",
    "\n",
    "# Ajout des mots personnalisés à la liste des stopwords\n",
    "sw.update(custom_sw)\n",
    "\n",
    "data_path = \"../data/txt_clean2\"\n",
    "\n",
    "if os.path.isdir(data_path):\n",
    "    files = os.listdir(data_path)\n",
    "    \n",
    "    combined_text = \"\"\n",
    "    for file_name in files:\n",
    "        file_path = os.path.join(data_path, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                file_text = file.read()\n",
    "                combined_text += file_text + \" \"\n",
    "    \n",
    "    # Créer un nuage de mots en excluant les stopwords\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white', stopwords=sw).generate(combined_text)\n",
    "\n",
    "    # Afficher le nuage de mots\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Le chemin spécifié n'est pas un répertoire valide.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Analyse entités nommées "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import sys\n",
    "import spacy\n",
    "from spacy.lang.fr.examples import sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('fr_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Chemin du dossier contenant les fichiers\n",
    "dossier = \"../data/txt_clean2\"\n",
    "\n",
    "# Chemin du dossier où sera enregistré le fichier de sortie\n",
    "dossier_sortie = \"../data\"\n",
    "\n",
    "# Nom du fichier à créer\n",
    "nom_fichier_sortie = \"fichier.txt\"\n",
    "\n",
    "# Chemin complet du fichier de sortie\n",
    "chemin_fichier_sortie = os.path.join(dossier_sortie, nom_fichier_sortie)\n",
    "\n",
    "# Ouvre le fichier de sortie en mode écriture\n",
    "with open(chemin_fichier_sortie, 'w') as fichier_sortie:\n",
    "    # Parcourt tous les fichiers du dossier\n",
    "    for nom_fichier in os.listdir(dossier):\n",
    "        chemin_fichier = os.path.join(dossier, nom_fichier)\n",
    "        # Vérifie si c'est bien un fichier texte\n",
    "        if os.path.isfile(chemin_fichier) and nom_fichier.endswith('.txt'):\n",
    "            # Lit le contenu du fichier et l'écrit dans le fichier de sortie\n",
    "            with open(chemin_fichier, 'r') as fichier_entree:\n",
    "                contenu = fichier_entree.read()\n",
    "                fichier_sortie.write(contenu)\n",
    "                fichier_sortie.write(\"\\n\")  # Ajoute une ligne entre chaque fichier\n",
    "\n",
    "print(f\"Le fichier {nom_fichier_sortie} a été créé avec succès dans le dossier 'data' en combinant les fichiers du dossier 'txt_clean'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le texte\n",
    "n=100000\n",
    "text = open(\"../data/fichier.txt\", encoding='utf-8').read()[:n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Traiter le texte\n",
    "\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compter les entités\n",
    "people = defaultdict(int)\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"PER\" and len(ent.text) > 3:\n",
    "        people[ent.text] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trier et imprimer\n",
    "\n",
    "sorted_people = sorted(people.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "for person, freq in sorted_people[:50]:\n",
    "    print(f\"{person} apparait {freq} fois dans le corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse de sentiment tous les 10 ans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from textblob import Blobber\n",
    "from textblob_fr import PatternTagger, PatternAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = Blobber(pos_tagger=PatternTagger(), analyzer=PatternAnalyzer())\n",
    "\n",
    "def get_sentiment(input_text):\n",
    "    blob = tb(input_text)\n",
    "    polarity, subjectivity = blob.sentiment\n",
    "    polarity_perc = f\"{100*abs(polarity):.0f}\"\n",
    "    subjectivity_perc = f\"{100*subjectivity:.0f}\"\n",
    "    if polarity > 0:\n",
    "        polarity_str = f\"{polarity_perc}% positive\"\n",
    "    elif polarity < 0:\n",
    "        polarity_str = f\"{polarity_perc}% negative\"\n",
    "    else:\n",
    "        polarity_str = \"neutral\"\n",
    "    if subjectivity > 0:\n",
    "        subjectivity_str = f\"{subjectivity}% subjective\"\n",
    "    else:\n",
    "        subjectivity_str = \"perfectly objective\"\n",
    "    print(f\"This text is {polarity_str} and {subjectivity_str}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Phrases de 1880"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment( \"il faudrait rendre la vaccination obligatoire en Belgique.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"Il faut que la vaccination soit obligatoire, comme elle l'est dans tous les pays de l'Europe, sauf en Belgique, en France, en Russie et en Turquie.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\" Il y a des instituts vaccinaux, et dans la pensée du gouvernement il s'agit d'établir en province des dépôts de vaccin où l'on pourra se le procurer gratuitement.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"En effet, dans toutes les zones géographiques où la vaccination et l'examen systématique des enfants ont été réalisés de manière sérieuse, où la vaccination a été appliquée sans exception, il n'a pas été nécessaire de rechercher, pendant une période d'un an, même durant une épidémie, un seul cas de décès causé par la variole.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrases de 1900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"Quant aux remèdes préventifs, il y en a deux ; l'un consiste dans la vaccination, l'autre dans l'emploi de mesures sanitaires propres à arrêter la contagion.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"La vaccination s'est peu répandue dans la pratique.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"A l'étranger, les pays qui ont décidé la vaccination obligatoire et où ia vaccination est généralisée, sont restés indemnes do cette infection  meutriere.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"Si le vaccin de la fièvre aphteuse est encore à .Touver, celui du rouget du porc est découvert iepuls longtemps, mais il n'eBt pas employé autant qu'il devrait l'être.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrases 1870"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"Or, d'après la nouvelle doctrine, la vaccination directe de l'animal malade à l'homme est seule un préservatif contre l'épouvantable variole et ie vaccin aurait dégénéré dans ces dernières années comme... comme fa littérature et l'art.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"Il prouve que ce vaccin est toujours aussi pur, aussi efficace, aussi préservateur que du temps de Jenner.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"Et en Belgique, il faudrait payer le vaccin provenant de la fabrique de l'Etat !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"La dépense que nécessite la vaccination des enfants pauvres est une dépense obligatoire pour les communes, en vertu de l'article 13 1 de la loi communale, et les députalions permanentes seraient fondées, même en l'absence de tout règlement provincial, it l'inscrire d'offleo aux budgets des communes, par application de l'article dont il s'agit.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrases 1890"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"Un jugement du tribunal de Francfort, sanctionne par un arrêt du tribunal suprême, a décidé que le fait de ne pas faire vacciner ses enfants pouvait donner lieu à une condamnation, mais qu'une fois cette condamnation subie, le pouvoir de l'autorité était épuisé et qu'elle ne pouvait contraindre les parents qui s'opposaient à la vaccination.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\" L'honorable sénateur de Liège avait préconisé aussi la vaccination obligatoire, invoquant à ce propos de nombreuses autorités\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"La muselière abolit la rage ; la vaccination et la revaccination obligatoires abolirent la variole.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"Un professeur de Varsovie a présenté un rapport sur la vaccination contre l'hydropho- bie et a exprimé la conviction que dans la plupart des cas la vaccination assure la guérison d.i malade.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse : clustering sur les 3 décennies de la période étudiée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import string\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from pprint import pprint\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/txt_clean2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Décénie 1870"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DECADE = '1870'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f for f in sorted(os.listdir(data_path)) if f\"_{DECADE[:-1]}\" in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de fichiers\n",
    "files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [open(data_path + f, \"r\", encoding=\"utf-8\").read() for f in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de textes\n",
    "texts[0][:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une fonction de pré-traitement\n",
    "def preprocessing(text, stem=True):\n",
    "    \"\"\" Tokenize text and remove punctuation \"\"\"\n",
    "    text = text.translate(string.punctuation)\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=preprocessing,\n",
    "    stop_words=stopwords.words('french'),\n",
    "    max_df=0.9,\n",
    "    min_df=1,\n",
    "    lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectors = vectorizer.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Détail de la matrice\n",
    "tfidf_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(\n",
    "    tfidf_vectors[0].toarray()[0],\n",
    "    index=vectorizer.get_feature_names_out()\n",
    "    ).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_array = tfidf_vectors.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLUSTERS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_model = KMeans(n_clusters=N_CLUSTERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = km_model.fit_predict(tfidf_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = collections.defaultdict(list)\n",
    "\n",
    "for idx, label in enumerate(clusters):\n",
    "    clustering[label].append(files[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(dict(clustering))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "reduced_vectors = pca.fit_transform(tfidf_vectors.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_vectors[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = reduced_vectors[:, 0]\n",
    "y_axis = reduced_vectors[:, 1]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "scatter = plt.scatter(x_axis, y_axis, s=100, c=clusters)\n",
    "\n",
    "# Ajouter les centroïdes\n",
    "centroids = pca.transform(km_model.cluster_centers_)\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1],  marker = \"x\", s=100, linewidths = 2, color='black')\n",
    "\n",
    "# Ajouter la légende\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=set(clusters), title=\"Clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Récupération des étiquettes de cluster pour chaque document\n",
    "labels = km_model.labels_\n",
    "\n",
    "# Créer un dictionnaire pour stocker les documents de chaque cluster\n",
    "clusters_documents = {}\n",
    "for i, label in enumerate(labels):\n",
    "    if label not in clusters_documents:\n",
    "        clusters_documents[label] = []\n",
    "    clusters_documents[label].append(i)  # Ajouter l'index du document au cluster correspondant\n",
    "\n",
    "for cluster_label, documents_indices in clusters_documents.items():\n",
    "    print(f\"Cluster {cluster_label}:\")\n",
    "    for doc_index in documents_indices:\n",
    "        print(texts[doc_index])  # Afficher le document correspondant à l'indice\n",
    "    print(\"\\n\")  # Ajouter un saut de ligne entre les clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yake\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantier l'extracteur de mots clés\n",
    "kw_extractor = yake.KeywordExtractor(lan=\"fr\", top=50)\n",
    "kw_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Charger les mots vides de la langue correspondante \n",
    "stop_words = set(stopwords.words(\"french\"))\n",
    "\n",
    "# Ajouter des mots à la liste des stop words\n",
    "additional_stop_words = [\"les\", \"plus\", \"cette\", \"fait\", \"faire\", \"être\", \"deux\", \"comme\", \"dont\", \"tout\",\n",
    "                        \"ils\", \"elles\", \"il\", \"elle\", \"bien\", \"sans\", \"peut\", \"tous\", \"après\", \"ainsi\", \"donc\", \"cet\", \"sous\",\n",
    "                        \"celle\", \"entre\", \"encore\", \"toutes\", \"pendant\", \"moins\", \"dire\", \"cela\", \"non\",\n",
    "                        \"faut\", \"trois\", \"aussi\", \"dit\", \"avoir\", \"doit\", \"contre\", \"depuis\", \"autres\",\n",
    "                        \"van\", \"het\", \"autre\", \"jusqu\", \"ville\", \"rossel\", \"dem\", \"si\", \"car\", \"autant\",\n",
    "                        \"toute\", \"très\", \"bien\", \"aucun\", \"comme\", \"celui\", \"chaque\", \"plusieurs\",\n",
    "                        \"toutes\", \"trop\", \"aucune\", \"parce\", \"quelques\", \"quel\", \"quelle\", \"quels\",\n",
    "                        \"quelles\", \"lorsqu\", \"lorsque\", \"dès\", \"dans\", \"leurs\", \"peu\", \"toute\", \"toutes\",\n",
    "                        \"DÈS\", \"près\", \"quart\", \"part\", \"ETC\", \"demi\", \"bas\", \"grand\", \"demande\", \"vers\",\n",
    "                        \"avant\", \"vers\",\"aussi\",\"ans\",\"leurs\",\"très\"]\n",
    "\n",
    "# Parcours chaque cluster\n",
    "for cluster_label, documents_indices in clusters_documents.items():\n",
    "    print(f\"Mots-clés fréquents pour le cluster {cluster_label}:\")\n",
    "    \n",
    "    # Collecter tous les mots non vides, non numériques et ne contenant pas une seule lettre de tous les documents du cluster\n",
    "    all_words = []\n",
    "    for doc_index in documents_indices:\n",
    "        text = texts[doc_index]\n",
    "        words = re.findall(r'\\b[A-Za-zÀ-ÿ]+\\b', text)  # Trouver les mots alphabétiques\n",
    "        # Filtrer les mots vides, les mots qui sont des chiffres et les mots d'une seule lettre\n",
    "        words = [word for word in words if word.lower() not in stop_words and not word.isdigit() and len(word) > 1]\n",
    "        all_words.extend(words)  # Ajouter les mots satisfaisant les critères à la liste\n",
    "    \n",
    "    # Calculer la fréquence des mots non vides, non numériques et ne contenant pas une seule lettre\n",
    "    word_freq = Counter(all_words)\n",
    "    \n",
    "    # Sélectionner les mots les plus fréquents\n",
    "    top_keywords = word_freq.most_common(200)\n",
    "    \n",
    "    # Afficher les mots-clés fréquents pour ce cluster\n",
    "    for keyword, frequency in top_keywords:\n",
    "        print(f\"Mot-clé: {keyword}, Fréquence: {frequency}\")\n",
    "    print(\"\\n\")  # Saut de ligne entre les clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parcours chaque cluster\n",
    "for cluster_label, documents_indices in clusters_documents.items():\n",
    "    print(f\"Nuage de mots pour le cluster {cluster_label}:\")\n",
    "    \n",
    "    \n",
    "    # Créer une chaîne de mots pour le nuage de mots\n",
    "    wordcloud_text = ' '.join([keyword for keyword, _ in top_keywords])\n",
    "    \n",
    "    # Créer et afficher le nuage de mots\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(wordcloud_text)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Nuage de mots pour le cluster {cluster_label}\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tac_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
