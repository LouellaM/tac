{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Première analyse : Mots-clés et nuage de mots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yake\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le répertoire qui contient vos fichiers txt exportés de  Camille\n",
    "indir = \"../data/camille_vaccination_/\"\n",
    "# Le répertoire qui contiendra les fichiers txt nettoyés\n",
    "outdir = \"../data/txt_clean2\"\n",
    "\n",
    "if not os.path.exists(outdir):\n",
    "    os.mkdir(outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = [\"vaccination\",\"vaccin\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une regex afin de trouver les mots de la liste query dans le texte\n",
    "regex = re.compile(f\"\\\\b({'|'.join(query)})\\\\b\", re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(indir)[:150]:\n",
    "    if file.endswith(\".txt\"):\n",
    "        relevant_sentences = []\n",
    "        f_in = open(os.path.join(indir, file), encoding=\"utf-8\")\n",
    "        text = f_in.read()\n",
    "        for sentence in sent_tokenize(text):\n",
    "            if regex.search(sentence):\n",
    "                relevant_sentences.append(sentence)\n",
    "        f_in.close()\n",
    "        f_out = open(os.path.join(outdir, file), \"w\", encoding=\"utf-8\")\n",
    "        f_out.write(\"\\n\\n\".join(relevant_sentences))\n",
    "        f_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantier l'extracteur de mots clés\n",
    "kw_extractor = yake.KeywordExtractor(lan=\"fr\", top=50)\n",
    "kw_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lister les Fichiers\n",
    "data_path = \"../data/txt_clean2\"\n",
    "files = os.listdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in sorted(files)[:1000]:\n",
    "    text = open(os.path.join(data_path, f), 'r', encoding=\"utf-8\").read()\n",
    "    keywords = kw_extractor.extract_keywords(text)\n",
    "    kept = []\n",
    "    for kw, score in keywords:\n",
    "        words = kw.split()\n",
    "        if len(words) == 2:\n",
    "            kept.append(kw)\n",
    "    print(f\"{f} mentions these keywords: {', '.join(kept)}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nuage de mots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import os\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords (Idem que dans s1)\n",
    "sw = stopwords.words(\"french\")\n",
    "sw += [\"les\", \"plus\", \"cette\", \"fait\", \"faire\", \"être\", \"deux\", \"comme\", \"dont\", \"tout\",\n",
    "       \"ils\", \"bien\", \"sans\", \"peut\", \"tous\", \"après\", \"ainsi\", \"donc\", \"cet\", \"sous\",\n",
    "       \"celle\", \"entre\", \"encore\", \"toutes\", \"pendant\", \"moins\", \"dire\", \"cela\", \"non\",\n",
    "       \"faut\", \"trois\", \"aussi\", \"dit\", \"avoir\", \"doit\", \"contre\", \"depuis\", \"autres\",\n",
    "       \"van\", \"het\", \"autre\", \"jusqu\", \"ville\", \"rossel\", \"dem\",\"rue\", \"villa\",\"louer\",\n",
    "       \"maison\",\"fr\",\"app\",\"ch\",\"vendre\",\"prix\",\"bel\",\"av\",\"jard\",\"Libre\",\"jardin\",\"vue\",\n",
    "       \"téléphone\",\"centre\",\"terrain\"]\n",
    "sw = set(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "\n",
    "# Télécharger les stopwords si ce n'est pas déjà fait\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Liste des stopwords du français\n",
    "sw = set(stopwords.words(\"french\"))\n",
    "\n",
    "# Ajout de mots personnalisés à la liste des stopwords\n",
    "custom_sw = [\n",
    "    \"les\", \"plus\", \"cette\", \"fait\", \"faire\", \"être\", \"deux\", \"comme\", \"dont\", \"tout\",\n",
    "    \"ils\", \"bien\", \"sans\", \"peut\", \"tous\", \"après\", \"ainsi\", \"donc\", \"cet\", \"sous\",\n",
    "    \"celle\", \"entre\", \"encore\", \"toutes\", \"pendant\", \"moins\", \"dire\", \"cela\", \"non\",\n",
    "    \"faut\", \"trois\", \"aussi\", \"dit\", \"avoir\", \"doit\", \"contre\", \"depuis\", \"autres\",\n",
    "    \"van\", \"het\", \"autre\", \"jusqu\", \"ville\", \"rossel\", \"dem\", \"rue\", \"villa\", \"louer\",\n",
    "    \"maison\", \"fr\", \"app\", \"ch\", \"vendre\", \"prix\", \"bel\", \"av\", \"jard\", \"Libre\", \"jardin\", \"do\"\n",
    "    \"a\", \"vue\", \"téléphone\", \"centre\", \"terrain\", \"déjà\",\"ot\",\"u\",\"qu'il\",\"or\", \"qu'une\",\"l'a\"\n",
    "]\n",
    "\n",
    "# Ajout des mots personnalisés à la liste des stopwords\n",
    "sw.update(custom_sw)\n",
    "\n",
    "data_path = \"../data/txt_clean2\"\n",
    "\n",
    "if os.path.isdir(data_path):\n",
    "    files = os.listdir(data_path)\n",
    "    \n",
    "    combined_text = \"\"\n",
    "    for file_name in files:\n",
    "        file_path = os.path.join(data_path, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                file_text = file.read()\n",
    "                combined_text += file_text + \" \"\n",
    "    \n",
    "    # Créer un nuage de mots en excluant les stopwords\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white', stopwords=sw).generate(combined_text)\n",
    "\n",
    "    # Afficher le nuage de mots\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Le chemin spécifié n'est pas un répertoire valide.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Analyse entités nommées "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import sys\n",
    "import spacy\n",
    "from spacy.lang.fr.examples import sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('fr_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Chemin du dossier contenant les fichiers\n",
    "dossier = \"../data/txt_clean2\"\n",
    "\n",
    "# Chemin du dossier où sera enregistré le fichier de sortie\n",
    "dossier_sortie = \"../data\"\n",
    "\n",
    "# Nom du fichier à créer\n",
    "nom_fichier_sortie = \"fichier.txt\"\n",
    "\n",
    "# Chemin complet du fichier de sortie\n",
    "chemin_fichier_sortie = os.path.join(dossier_sortie, nom_fichier_sortie)\n",
    "\n",
    "# Ouvre le fichier de sortie en mode écriture\n",
    "with open(chemin_fichier_sortie, 'w') as fichier_sortie:\n",
    "    # Parcourt tous les fichiers du dossier\n",
    "    for nom_fichier in os.listdir(dossier):\n",
    "        chemin_fichier = os.path.join(dossier, nom_fichier)\n",
    "        # Vérifie si c'est bien un fichier texte\n",
    "        if os.path.isfile(chemin_fichier) and nom_fichier.endswith('.txt'):\n",
    "            # Lit le contenu du fichier et l'écrit dans le fichier de sortie\n",
    "            with open(chemin_fichier, 'r') as fichier_entree:\n",
    "                contenu = fichier_entree.read()\n",
    "                fichier_sortie.write(contenu)\n",
    "                fichier_sortie.write(\"\\n\")  # Ajoute une ligne entre chaque fichier\n",
    "\n",
    "print(f\"Le fichier {nom_fichier_sortie} a été créé avec succès dans le dossier 'data' en combinant les fichiers du dossier 'txt_clean'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le texte\n",
    "n=100000\n",
    "text = open(\"../data/fichier.txt\", encoding='utf-8').read()[:n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Traiter le texte\n",
    "\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compter les entités\n",
    "people = defaultdict(int)\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"PER\" and len(ent.text) > 3:\n",
    "        people[ent.text] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trier et imprimer\n",
    "\n",
    "sorted_people = sorted(people.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "for person, freq in sorted_people[:50]:\n",
    "    print(f\"{person} apparait {freq} fois dans le corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse de sentiment tous les 10 ans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from textblob import Blobber\n",
    "from textblob_fr import PatternTagger, PatternAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = Blobber(pos_tagger=PatternTagger(), analyzer=PatternAnalyzer())\n",
    "\n",
    "def get_sentiment(input_text):\n",
    "    blob = tb(input_text)\n",
    "    polarity, subjectivity = blob.sentiment\n",
    "    polarity_perc = f\"{100*abs(polarity):.0f}\"\n",
    "    subjectivity_perc = f\"{100*subjectivity:.0f}\"\n",
    "    if polarity > 0:\n",
    "        polarity_str = f\"{polarity_perc}% positive\"\n",
    "    elif polarity < 0:\n",
    "        polarity_str = f\"{polarity_perc}% negative\"\n",
    "    else:\n",
    "        polarity_str = \"neutral\"\n",
    "    if subjectivity > 0:\n",
    "        subjectivity_str = f\"{subjectivity}% subjective\"\n",
    "    else:\n",
    "        subjectivity_str = \"perfectly objective\"\n",
    "    print(f\"This text is {polarity_str} and {subjectivity_str}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Phrases de 1880"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment( \"il faudrait rendre la vaccination obligatoire en Belgique.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"Il faut que la vaccination soit obligatoire, comme elle l'est dans tous les pays de l'Europe, sauf en Belgique, en France, en Russie et en Turquie.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\" Il y a des instituts vaccinaux, et dans la pensée du gouvernement il s'agit d'établir en province des dépôts de vaccin où l'on pourra se le procurer gratuitement.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"En effet, dans toutes les zones géographiques où la vaccination et l'examen systématique des enfants ont été réalisés de manière sérieuse, où la vaccination a été appliquée sans exception, il n'a pas été nécessaire de rechercher, pendant une période d'un an, même durant une épidémie, un seul cas de décès causé par la variole.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrases de 1900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"Quant aux remèdes préventifs, il y en a deux ; l'un consiste dans la vaccination, l'autre dans l'emploi de mesures sanitaires propres à arrêter la contagion.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"La vaccination s'est peu répandue dans la pratique.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"A l'étranger, les pays qui ont décidé la vaccination obligatoire et où ia vaccination est généralisée, sont restés indemnes do cette infection  meutriere.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"Si le vaccin de la fièvre aphteuse est encore à .Touver, celui du rouget du porc est découvert iepuls longtemps, mais il n'eBt pas employé autant qu'il devrait l'être.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrases 1870"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"Or, d'après la nouvelle doctrine, la vaccination directe de l'animal malade à l'homme est seule un préservatif contre l'épouvantable variole et ie vaccin aurait dégénéré dans ces dernières années comme... comme fa littérature et l'art.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"Il prouve que ce vaccin est toujours aussi pur, aussi efficace, aussi préservateur que du temps de Jenner.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"Et en Belgique, il faudrait payer le vaccin provenant de la fabrique de l'Etat !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"La dépense que nécessite la vaccination des enfants pauvres est une dépense obligatoire pour les communes, en vertu de l'article 13 1 de la loi communale, et les députalions permanentes seraient fondées, même en l'absence de tout règlement provincial, it l'inscrire d'offleo aux budgets des communes, par application de l'article dont il s'agit.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrases 1890"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"Un jugement du tribunal de Francfort, sanctionne par un arrêt du tribunal suprême, a décidé que le fait de ne pas faire vacciner ses enfants pouvait donner lieu à une condamnation, mais qu'une fois cette condamnation subie, le pouvoir de l'autorité était épuisé et qu'elle ne pouvait contraindre les parents qui s'opposaient à la vaccination.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\" L'honorable sénateur de Liège avait préconisé aussi la vaccination obligatoire, invoquant à ce propos de nombreuses autorités\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"La muselière abolit la rage ; la vaccination et la revaccination obligatoires abolirent la variole.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"Un professeur de Varsovie a présenté un rapport sur la vaccination contre l'hydropho- bie et a exprimé la conviction que dans la plupart des cas la vaccination assure la guérison d.i malade.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse : clustering sur les 3 parties (début, milieu, fin) de la période étudiée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Assurez-vous de télécharger les stop words français une seule fois\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Chemin vers les données\n",
    "data_path = \"../data/txt_clean2\"\n",
    "\n",
    "# Lire les fichiers\n",
    "file_names = os.listdir(data_path)\n",
    "\n",
    "# Diviser en trois parties égales\n",
    "third = len(file_names) // 3\n",
    "\n",
    "corpus_part1 = [os.path.join(data_path, file_name) for file_name in file_names[:third]]\n",
    "corpus_part2 = [os.path.join(data_path, file_name) for file_name in file_names[third:2*third]]\n",
    "corpus_part3 = [os.path.join(data_path, file_name) for file_name in file_names[2*third:]]\n",
    "\n",
    "# Utilisation des stop words français\n",
    "stop_words = list(stopwords.words('french'))\n",
    "\n",
    "def perform_clustering(corpus):\n",
    "    data = []\n",
    "    for file_name in corpus:\n",
    "        with open(file_name, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            data.append(text)\n",
    "\n",
    "    # Vectorisation des données textuelles avec les stop words français\n",
    "    vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "    X = vectorizer.fit_transform(data)\n",
    "\n",
    "    # Clustering avec K-Means\n",
    "    num_clusters = 2 # Vous pouvez ajuster le nombre de clusters\n",
    "    kmeans = KMeans(n_clusters=num_clusters)\n",
    "    kmeans.fit(X)\n",
    "\n",
    "    # Réduction de dimensionnalité avec PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced_features = pca.fit_transform(X.toarray())\n",
    "\n",
    "    # Obtention des étiquettes des clusters et des centres des clusters\n",
    "    labels = kmeans.predict(X)\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "\n",
    "    # Tracer les clusters\n",
    "    for i in range(num_clusters):\n",
    "        plt.scatter(reduced_features[labels == i, 0], reduced_features[labels == i, 1], label=f'Cluster {i+1}')\n",
    "\n",
    "    # Tracer les centres des clusters\n",
    "    plt.scatter(cluster_centers[:, 0], cluster_centers[:, 1], marker='x', color='red', s=200, label='Centres des clusters')\n",
    "\n",
    "    plt.title('Clustering des Textes')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Effectuer le clustering pour la première partie du corpus\n",
    "print(\"Clustering pour la première partie du corpus :\")\n",
    "perform_clustering(corpus_part1)\n",
    "\n",
    "# Effectuer le clustering pour la deuxième partie du corpus\n",
    "print(\"Clustering pour la deuxième partie du corpus :\")\n",
    "perform_clustering(corpus_part2)\n",
    "\n",
    "# Effectuer le clustering pour la troisième partie du corpus\n",
    "print(\"Clustering pour la troisième partie du corpus :\")\n",
    "perform_clustering(corpus_part3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mots-clés et fréquences des clusters des 3 parties du corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "# Assurez-vous de télécharger les stop words français une seule fois\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Chemin vers les données\n",
    "data_path = \"../data/txt_clean2\"\n",
    "\n",
    "# Lire les fichiers\n",
    "file_names = os.listdir(data_path)\n",
    "\n",
    "# Diviser en trois parties égales\n",
    "third = len(file_names) // 3\n",
    "\n",
    "corpus_part1 = [os.path.join(data_path, file_name) for file_name in file_names[:third]]\n",
    "corpus_part2 = [os.path.join(data_path, file_name) for file_name in file_names[third:2*third]]\n",
    "corpus_part3 = [os.path.join(data_path, file_name) for file_name in file_names[2*third:]]\n",
    "\n",
    "# Utilisation des stop words français de NLTK\n",
    "stop_words = set(stopwords.words('french'))\n",
    "\n",
    "# Liste étendue de stop words\n",
    "extra_stopwords = [\n",
    "    \"les\", \"plus\", \"cette\", \"fait\", \"faire\", \"être\", \"deux\", \"comme\", \"dont\", \"tout\",\n",
    "    \"ils\", \"bien\", \"sans\", \"peut\", \"tous\", \"après\", \"ainsi\", \"donc\", \"cet\", \"sous\",\n",
    "    \"celle\", \"entre\", \"encore\", \"toutes\", \"pendant\", \"moins\", \"dire\", \"cela\", \"non\",\n",
    "    \"faut\", \"trois\", \"aussi\", \"dit\", \"avoir\", \"doit\", \"contre\", \"depuis\", \"autres\",\n",
    "    \"van\", \"het\", \"autre\", \"jusqu\", \"ville\", \"rossel\", \"dem\", \"rue\", \"villa\", \"louer\",\n",
    "    \"maison\", \"fr\", \"app\", \"ch\", \"vendre\", \"prix\", \"bel\", \"av\", \"jard\", \"Libre\", \"jardin\",\n",
    "    \"vue\", \"téléphone\", \"centre\", \"terrain\"\n",
    "]\n",
    "\n",
    "# Ajouter les mots de la liste étendue à la liste de stopwords\n",
    "stop_words.update(extra_stopwords)\n",
    "\n",
    "# Convertir l'ensemble stop_words en une liste\n",
    "stop_words_list = list(stop_words)\n",
    "\n",
    "def perform_clustering(corpus):\n",
    "    data = []\n",
    "    for file_name in corpus:\n",
    "        with open(file_name, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            data.append(text)\n",
    "\n",
    "    # Utiliser la liste comme stop words dans TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer(stop_words=stop_words_list)\n",
    "    X = vectorizer.fit_transform(data)\n",
    "\n",
    "    # Clustering avec K-Means\n",
    "    num_clusters = 2  # Vous pouvez ajuster le nombre de clusters\n",
    "    kmeans = KMeans(n_clusters=num_clusters)\n",
    "    kmeans.fit(X)\n",
    "\n",
    "    # Obtention des étiquettes des clusters et des centres des clusters\n",
    "    labels = kmeans.predict(X)\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "\n",
    "    # Réduction de dimensionnalité avec PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced_features = pca.fit_transform(X.toarray())\n",
    "\n",
    "    # Tracer les clusters\n",
    "    for i in range(num_clusters):\n",
    "        plt.scatter(reduced_features[labels == i, 0], reduced_features[labels == i, 1], label=f'Cluster {i+1}')\n",
    "\n",
    "    # Tracer les centres des clusters\n",
    "    plt.scatter(cluster_centers[:, 0], cluster_centers[:, 1], marker='x', color='red', s=200, label='Centres des clusters')\n",
    "\n",
    "    plt.title('Clustering des Textes')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Regroupement des textes par cluster pour obtenir les mots-clés\n",
    "    texts_by_cluster = {}\n",
    "    for i, label in enumerate(labels):\n",
    "        if label not in texts_by_cluster:\n",
    "            texts_by_cluster[label] = []\n",
    "        texts_by_cluster[label].append(data[i])\n",
    "\n",
    "    # Obtenir les termes les plus fréquents par cluster\n",
    "    keywords_by_cluster = {}\n",
    "    for cluster, texts in texts_by_cluster.items():\n",
    "        all_text = ' '.join(texts)\n",
    "        tokens = nltk.word_tokenize(all_text)\n",
    "        filtered_tokens = [token.lower() for token in tokens if token.isalnum() and token.lower() not in stop_words_list]\n",
    "        word_freq = Counter(filtered_tokens)\n",
    "        top_keywords = word_freq.most_common(20)  # Vous pouvez ajuster le nombre de mots-clés à afficher\n",
    "        keywords_by_cluster[cluster] = top_keywords\n",
    "\n",
    "    return keywords_by_cluster\n",
    "\n",
    "# Effectuer le clustering pour la première partie du corpus\n",
    "print(\"Clustering pour la première partie du corpus :\")\n",
    "keywords_part1 = perform_clustering(corpus_part1)\n",
    "print(\"Mots-clés pour la première partie du corpus :\")\n",
    "print(keywords_part1)\n",
    "\n",
    "# Effectuer le clustering pour la deuxième partie du corpus\n",
    "print(\"\\nClustering pour la deuxième partie du corpus :\")\n",
    "keywords_part2 = perform_clustering(corpus_part2)\n",
    "print(\"Mots-clés pour la deuxième partie du corpus :\")\n",
    "print(keywords_part2)\n",
    "\n",
    "# Effectuer le clustering pour la troisième partie du corpus\n",
    "print(\"\\nClustering pour la troisième partie du corpus :\")\n",
    "keywords_part3 = perform_clustering(corpus_part3)\n",
    "print(\"Mots-clés pour la troisième partie du corpus :\")\n",
    "print(keywords_part3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nuage de mots pour les clusters de chaque partie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fonction pour créer un nuage de mots pour chaque cluster dans chaque partie\n",
    "def create_wordclouds(keywords):\n",
    "    for part_num, clusters in enumerate(keywords, 1):\n",
    "        for cluster_num, words in clusters.items():\n",
    "            wordcloud = WordCloud(width=800, height=400, background_color='white',\n",
    "                                  max_words=100, collocations=False).generate_from_frequencies(dict(words))\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.imshow(wordcloud, interpolation='bilinear')\n",
    "            plt.axis('off')\n",
    "            plt.title(f\"Partie {part_num}, Cluster {cluster_num}\")\n",
    "            plt.show()\n",
    "\n",
    "# Utilisation pour générer les nuages de mots avec 100 mots\n",
    "create_wordclouds([keywords_part1, keywords_part2, keywords_part3])\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tac_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
